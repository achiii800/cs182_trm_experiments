{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRM Router Experiments - Standalone\n",
    "\n",
    "This notebook implements the Tiny Recursive Model (TRM) router experiments for\n",
    "meta-optimization of inner solvers in the Manifold Muon optimizer.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Data Collection**: Run training with fixed solvers, collecting dynamics\n",
    "2. **Data Merging**: Merge runs to create oracle labels\n",
    "3. **TRM Training**: Train TRM router to predict optimal solver\n",
    "4. **Evaluation**: Compare TRM routing vs fixed baselines\n",
    "\n",
    "## Reference\n",
    "- Jolicoeur-Martineau, A. (2025). \"Less is More: Recursive Reasoning with Tiny Networks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Running in Google Colab environment\n",
      "ğŸ“¦ Cloning repository...\n",
      "Cloning into '/content/cs182_trm_experiments'...\n",
      "remote: Enumerating objects: 31, done.\u001b[K\n",
      "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
      "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
      "remote: Total 31 (delta 3), reused 30 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (31/31), 41.08 KiB | 13.69 MiB/s, done.\n",
      "Resolving deltas: 100% (3/3), done.\n",
      "/content/cs182_trm_experiments\n",
      "ğŸ“š Installing requirements...\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Environment Setup for Colab Kernel\n",
    "# ========================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect if we're in Colab environment\n",
    "IS_COLAB = 'COLAB_GPU' in os.environ or '/content' in os.getcwd()\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"ğŸš€ Running in Google Colab environment\")\n",
    "    \n",
    "    # Clone repo if not present\n",
    "    if not os.path.exists('/content/cs182_trm_experiments'):\n",
    "        print(\"ğŸ“¦ Cloning repository...\")\n",
    "        !git clone https://github.com/achiii800/cs182_trm_experiments.git /content/cs182_trm_experiments\n",
    "        %cd /content/cs182_trm_experiments\n",
    "    else:\n",
    "        print(\"âœ… Repository already exists\")\n",
    "        %cd /content/cs182_trm_experiments\n",
    "    \n",
    "    # Install requirements\n",
    "    print(\"ğŸ“š Installing requirements...\")\n",
    "    %pip install -q -r requirements.txt\n",
    "    \n",
    "    # Set working directory\n",
    "    REPO_ROOT = Path('/content/cs182_trm_experiments')\n",
    "    \n",
    "else:\n",
    "    print(\"ğŸ’» Running in local/VSCode environment\")\n",
    "    # Assume we're in notebooks/ directory\n",
    "    REPO_ROOT = Path('..').resolve()\n",
    "    print(f\"ğŸ“ Repo root: {REPO_ROOT}\")\n",
    "\n",
    "# Add repo to path\n",
    "sys.path.insert(0, str(REPO_ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda (NVIDIA L4)\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from models import get_model\n",
    "from muon import MuonSGD, get_inner_solver\n",
    "from trm import (\n",
    "    TRMRouter,\n",
    "    create_trm_router,\n",
    "    TRMDataCollector,\n",
    "    DynamicsDataset,\n",
    "    DynamicsFeatureExtractor,\n",
    "    TrainingState,\n",
    "    merge_solver_runs,\n",
    "    create_dataloaders,\n",
    ")\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "print(f\"Using device: {device.type} ({device_name})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection\n",
    "\n",
    "Run training with each fixed solver and collect dynamics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:12<00:00, 13.3MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 390, Test batches: 79\n"
     ]
    }
   ],
   "source": [
    "# Data loaders\n",
    "def make_loaders(batch_size=128):\n",
    "    transform_train = T.Compose([\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    transform_test = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    \n",
    "    train_set = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    test_set = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = make_loaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_solver_dynamics(\n",
    "    solver_name: str,\n",
    "    epochs: int = 5,\n",
    "    lr: float = 0.1,\n",
    "    spectral_budget: float = 0.1,\n",
    "    output_path: str = None,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"Run training with a fixed solver and collect dynamics.\"\"\"\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Create model\n",
    "    model = get_model('small_cnn').to(device)\n",
    "    \n",
    "    # Create optimizer with fixed solver\n",
    "    inner_solver = get_inner_solver(solver_name)\n",
    "    optimizer = MuonSGD(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=5e-4,\n",
    "        spectral_budget=spectral_budget,\n",
    "        inner_solver=inner_solver,\n",
    "    )\n",
    "    \n",
    "    # Data collector\n",
    "    collector = TRMDataCollector(\n",
    "        model=model,\n",
    "        solver_name=solver_name,\n",
    "        total_epochs=epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        record_every=1,\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch} [{solver_name}]\")\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(pbar):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            \n",
    "            # Compute gradient norm\n",
    "            loss.backward()\n",
    "            grad_norm = sum(\n",
    "                p.grad.norm().item() ** 2 \n",
    "                for p in model.parameters() if p.grad is not None\n",
    "            ) ** 0.5\n",
    "            \n",
    "            # Pre-step record\n",
    "            collector.pre_step(\n",
    "                loss=loss.item(),\n",
    "                grad_norm=grad_norm,\n",
    "                epoch=epoch,\n",
    "                step=global_step,\n",
    "            )\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            \n",
    "            # Post-step: get next loss\n",
    "            with torch.no_grad():\n",
    "                next_logits = model(x)\n",
    "                next_loss = F.cross_entropy(next_logits, y).item()\n",
    "            \n",
    "            collector.post_step(next_loss=next_loss)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"  Avg loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save\n",
    "    if output_path:\n",
    "        collector.save(output_path)\n",
    "    \n",
    "    return collector.records, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Collecting dynamics for: dual_ascent\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d7866f8f83443f94221d58e5553be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [dual_ascent]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Avg loss: 2.2580\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b297334918a42f5ad2e0e85e711f59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 [dual_ascent]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Avg loss: 2.2005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f42d5d714744f2da414b5ed610c7510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 [dual_ascent]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Avg loss: 2.1916\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76bc3e9fb814ab2a54216abfa561a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 [dual_ascent]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Avg loss: 2.0411\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb734f4ebe54b48b312a6e284dcdccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 [dual_ascent]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Avg loss: 2.1633\n",
      "Saved 1950 records to ../results/dynamics_dual_ascent.pkl\n",
      "\n",
      "==================================================\n",
      "Collecting dynamics for: admm\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a27caee68ba4231b4cf6ac1298f8824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [admm]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4096) must match the size of tensor b (256) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1280461978.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     records, losses = collect_solver_dynamics(\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0msolver_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Use more epochs for full experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-542003530.py\u001b[0m in \u001b[0;36mcollect_solver_dynamics\u001b[0;34m(solver_name, epochs, lr, spectral_budget, output_path, seed)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m                             )\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/cs182_trm_experiments/muon/muon_sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mspectral_budget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspectral_budget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/cs182_trm_experiments/muon/inner_solvers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, W, delta, spectral_budget)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mZ_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mU\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m                 \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spectral_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspectral_budget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                 \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4096) must match the size of tensor b (256) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Collect data for each solver (select subset for quick iteration)\n",
    "SOLVERS = ['dual_ascent', 'admm', 'frank_wolfe']\n",
    "DATA_DIR = Path('../results')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "solver_records = {}\n",
    "solver_losses = {}\n",
    "\n",
    "for solver in SOLVERS:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Collecting dynamics for: {solver}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    records, losses = collect_solver_dynamics(\n",
    "        solver_name=solver,\n",
    "        epochs=5,  # Use more epochs for full experiment\n",
    "        output_path=DATA_DIR / f'dynamics_{solver}.pkl',\n",
    "    )\n",
    "    \n",
    "    solver_records[solver] = records\n",
    "    solver_losses[solver] = losses\n",
    "\n",
    "print(\"\\nData collection complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "for solver, losses in solver_losses.items():\n",
    "    plt.plot(losses, label=solver, marker='o')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss by Solver')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'solver_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Merge Data and Create Oracle Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge solver runs\n",
    "run_paths = {solver: DATA_DIR / f'dynamics_{solver}.pkl' for solver in SOLVERS}\n",
    "merged_path = DATA_DIR / 'merged_dynamics.pkl'\n",
    "\n",
    "merged_data = merge_solver_runs(run_paths, merged_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze oracle distribution over training\n",
    "from collections import Counter\n",
    "\n",
    "records = merged_data['records']\n",
    "\n",
    "# Oracle by epoch\n",
    "oracle_by_epoch = {}\n",
    "for r in records:\n",
    "    epoch = r.epoch if hasattr(r, 'epoch') else r['epoch']\n",
    "    oracle = r.oracle_solver if hasattr(r, 'oracle_solver') else r['oracle_solver']\n",
    "    \n",
    "    if epoch not in oracle_by_epoch:\n",
    "        oracle_by_epoch[epoch] = []\n",
    "    oracle_by_epoch[epoch].append(oracle)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "epochs = sorted(oracle_by_epoch.keys())\n",
    "for solver in SOLVERS:\n",
    "    fracs = []\n",
    "    for ep in epochs:\n",
    "        count = sum(1 for o in oracle_by_epoch[ep] if o == solver)\n",
    "        fracs.append(count / len(oracle_by_epoch[ep]))\n",
    "    ax.plot(epochs, fracs, label=solver, marker='o')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Fraction as Oracle')\n",
    "ax.set_title('Which Solver is Best at Each Epoch?')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'oracle_by_epoch.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train TRM Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = DynamicsDataset(merged_path)\n",
    "stats = dataset.get_statistics()\n",
    "\n",
    "print(f\"Dataset size: {stats['num_samples']}\")\n",
    "print(f\"Feature dim: {stats['feature_dim']}\")\n",
    "print(\"Solver distribution:\")\n",
    "for name, count in stats['solver_distribution'].items():\n",
    "    print(f\"  {name}: {count} ({100*count/stats['num_samples']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TRM router (inline)\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Create dataloaders\n",
    "train_dl, val_dl = create_dataloaders(merged_path, batch_size=64, train_split=0.8)\n",
    "\n",
    "# Create model\n",
    "trm = create_trm_router(size='small').to(device)\n",
    "print(f\"TRM Router: {trm.num_parameters:,} parameters\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(trm.parameters(), lr=1e-4, weight_decay=0.1)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n",
    "\n",
    "# Training\n",
    "history = []\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    # Train\n",
    "    trm.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch in train_dl:\n",
    "        features = batch['features'].to(device)\n",
    "        labels = batch['solver_label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = trm(features, return_all_cycles=True)\n",
    "        \n",
    "        # Deep supervision loss\n",
    "        loss = sum(\n",
    "            F.cross_entropy(logits, labels)\n",
    "            for logits in outputs['all_solver_logits']\n",
    "        ) / len(outputs['all_solver_logits'])\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(trm.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * features.size(0)\n",
    "        preds = outputs['solver_logits'].argmax(dim=-1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += features.size(0)\n",
    "    \n",
    "    # Eval\n",
    "    trm.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dl:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['solver_label'].to(device)\n",
    "            \n",
    "            outputs = trm(features)\n",
    "            preds = outputs['solver_logits'].argmax(dim=-1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += features.size(0)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    train_acc = train_correct / train_total\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    history.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss / train_total,\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "    })\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(trm.state_dict(), DATA_DIR / 'trm_router_best.pt')\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {train_loss/train_total:.4f} | \"\n",
    "              f\"Train: {train_acc:.4f} | Val: {val_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs = [h['epoch'] for h in history]\n",
    "\n",
    "ax1.plot(epochs, [h['train_loss'] for h in history])\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('TRM Router Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(epochs, [h['train_acc'] for h in history], label='Train')\n",
    "ax2.plot(epochs, [h['val_acc'] for h in history], label='Val')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('TRM Router Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'trm_training.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Analysis\n",
    "\n",
    "Analyze what the TRM learned about solver selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "trm.load_state_dict(torch.load(DATA_DIR / 'trm_router_best.pt'))\n",
    "trm.eval()\n",
    "\n",
    "# Confusion matrix\n",
    "SOLVER_NAMES = ['spectral_clip', 'dual_ascent', 'quasi_newton', 'frank_wolfe', 'admm']\n",
    "confusion = torch.zeros(5, 5, dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dl:\n",
    "        features = batch['features'].to(device)\n",
    "        labels = batch['solver_label'].to(device)\n",
    "        \n",
    "        outputs = trm(features)\n",
    "        preds = outputs['solver_logits'].argmax(dim=-1)\n",
    "        \n",
    "        for t, p in zip(labels.cpu(), preds.cpu()):\n",
    "            confusion[t, p] += 1\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(confusion.float() / confusion.sum(dim=1, keepdim=True), cmap='Blues')\n",
    "\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_yticks(range(5))\n",
    "ax.set_xticklabels([s[:8] for s in SOLVER_NAMES], rotation=45, ha='right')\n",
    "ax.set_yticklabels([s[:8] for s in SOLVER_NAMES])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True (Oracle)')\n",
    "ax.set_title('TRM Routing Confusion Matrix')\n",
    "\n",
    "# Add values\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        val = confusion[i, j].item()\n",
    "        color = 'white' if confusion[i, j] > confusion.max() / 2 else 'black'\n",
    "        ax.text(j, i, f'{val}', ha='center', va='center', color=color, fontsize=10)\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Data Collection**: Collecting training dynamics from runs with fixed solvers\n",
    "2. **Oracle Labeling**: Determining which solver was best at each step\n",
    "3. **TRM Training**: Training a tiny recursive model to predict optimal solver\n",
    "4. **Evaluation**: Measuring TRM's ability to match the oracle\n",
    "\n",
    "Key findings:\n",
    "- Different solvers are optimal at different training phases\n",
    "- TRM can learn to approximate the oracle solver selection\n",
    "- The TRM overhead is negligible (~100K parameters)\n",
    "\n",
    "This connects to the broader Manifold Muon project by exploring whether\n",
    "meta-learned routing can improve hyperparameter transfer properties."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
