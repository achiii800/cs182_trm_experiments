{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRM Router Experiments - Standalone\n",
    "\n",
    "This notebook implements the Tiny Recursive Model (TRM) router experiments for\n",
    "meta-optimization of inner solvers in the Manifold Muon optimizer.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Data Collection**: Run training with fixed solvers, collecting dynamics\n",
    "2. **Data Merging**: Merge runs to create oracle labels\n",
    "3. **TRM Training**: Train TRM router to predict optimal solver\n",
    "4. **Evaluation**: Compare TRM routing vs fixed baselines\n",
    "\n",
    "## Reference\n",
    "- Jolicoeur-Martineau, A. (2025). \"Less is More: Recursive Reasoning with Tiny Networks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from models import get_model\n",
    "from muon import MuonSGD, get_inner_solver\n",
    "from trm import (\n",
    "    TRMRouter,\n",
    "    create_trm_router,\n",
    "    TRMDataCollector,\n",
    "    DynamicsDataset,\n",
    "    DynamicsFeatureExtractor,\n",
    "    TrainingState,\n",
    "    merge_solver_runs,\n",
    "    create_dataloaders,\n",
    ")\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection\n",
    "\n",
    "Run training with each fixed solver and collect dynamics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "def make_loaders(batch_size=128):\n",
    "    transform_train = T.Compose([\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    transform_test = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    \n",
    "    train_set = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    test_set = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = make_loaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_solver_dynamics(\n",
    "    solver_name: str,\n",
    "    epochs: int = 5,\n",
    "    lr: float = 0.1,\n",
    "    spectral_budget: float = 0.1,\n",
    "    output_path: str = None,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"Run training with a fixed solver and collect dynamics.\"\"\"\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Create model\n",
    "    model = get_model('small_cnn').to(device)\n",
    "    \n",
    "    # Create optimizer with fixed solver\n",
    "    inner_solver = get_inner_solver(solver_name)\n",
    "    optimizer = MuonSGD(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=5e-4,\n",
    "        spectral_budget=spectral_budget,\n",
    "        inner_solver=inner_solver,\n",
    "    )\n",
    "    \n",
    "    # Data collector\n",
    "    collector = TRMDataCollector(\n",
    "        model=model,\n",
    "        solver_name=solver_name,\n",
    "        total_epochs=epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        record_every=1,\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch} [{solver_name}]\")\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(pbar):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            \n",
    "            # Compute gradient norm\n",
    "            loss.backward()\n",
    "            grad_norm = sum(\n",
    "                p.grad.norm().item() ** 2 \n",
    "                for p in model.parameters() if p.grad is not None\n",
    "            ) ** 0.5\n",
    "            \n",
    "            # Pre-step record\n",
    "            collector.pre_step(\n",
    "                loss=loss.item(),\n",
    "                grad_norm=grad_norm,\n",
    "                epoch=epoch,\n",
    "                step=global_step,\n",
    "            )\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            \n",
    "            # Post-step: get next loss\n",
    "            with torch.no_grad():\n",
    "                next_logits = model(x)\n",
    "                next_loss = F.cross_entropy(next_logits, y).item()\n",
    "            \n",
    "            collector.post_step(next_loss=next_loss)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"  Avg loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save\n",
    "    if output_path:\n",
    "        collector.save(output_path)\n",
    "    \n",
    "    return collector.records, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data for each solver (select subset for quick iteration)\n",
    "SOLVERS = ['dual_ascent', 'admm', 'frank_wolfe']\n",
    "DATA_DIR = Path('../results')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "solver_records = {}\n",
    "solver_losses = {}\n",
    "\n",
    "for solver in SOLVERS:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Collecting dynamics for: {solver}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    records, losses = collect_solver_dynamics(\n",
    "        solver_name=solver,\n",
    "        epochs=5,  # Use more epochs for full experiment\n",
    "        output_path=DATA_DIR / f'dynamics_{solver}.pkl',\n",
    "    )\n",
    "    \n",
    "    solver_records[solver] = records\n",
    "    solver_losses[solver] = losses\n",
    "\n",
    "print(\"\\nData collection complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "for solver, losses in solver_losses.items():\n",
    "    plt.plot(losses, label=solver, marker='o')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss by Solver')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'solver_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Merge Data and Create Oracle Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge solver runs\n",
    "run_paths = {solver: DATA_DIR / f'dynamics_{solver}.pkl' for solver in SOLVERS}\n",
    "merged_path = DATA_DIR / 'merged_dynamics.pkl'\n",
    "\n",
    "merged_data = merge_solver_runs(run_paths, merged_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze oracle distribution over training\n",
    "from collections import Counter\n",
    "\n",
    "records = merged_data['records']\n",
    "\n",
    "# Oracle by epoch\n",
    "oracle_by_epoch = {}\n",
    "for r in records:\n",
    "    epoch = r.epoch if hasattr(r, 'epoch') else r['epoch']\n",
    "    oracle = r.oracle_solver if hasattr(r, 'oracle_solver') else r['oracle_solver']\n",
    "    \n",
    "    if epoch not in oracle_by_epoch:\n",
    "        oracle_by_epoch[epoch] = []\n",
    "    oracle_by_epoch[epoch].append(oracle)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "epochs = sorted(oracle_by_epoch.keys())\n",
    "for solver in SOLVERS:\n",
    "    fracs = []\n",
    "    for ep in epochs:\n",
    "        count = sum(1 for o in oracle_by_epoch[ep] if o == solver)\n",
    "        fracs.append(count / len(oracle_by_epoch[ep]))\n",
    "    ax.plot(epochs, fracs, label=solver, marker='o')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Fraction as Oracle')\n",
    "ax.set_title('Which Solver is Best at Each Epoch?')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'oracle_by_epoch.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train TRM Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = DynamicsDataset(merged_path)\n",
    "stats = dataset.get_statistics()\n",
    "\n",
    "print(f\"Dataset size: {stats['num_samples']}\")\n",
    "print(f\"Feature dim: {stats['feature_dim']}\")\n",
    "print(\"Solver distribution:\")\n",
    "for name, count in stats['solver_distribution'].items():\n",
    "    print(f\"  {name}: {count} ({100*count/stats['num_samples']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TRM router (inline)\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Create dataloaders\n",
    "train_dl, val_dl = create_dataloaders(merged_path, batch_size=64, train_split=0.8)\n",
    "\n",
    "# Create model\n",
    "trm = create_trm_router(size='small').to(device)\n",
    "print(f\"TRM Router: {trm.num_parameters:,} parameters\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(trm.parameters(), lr=1e-4, weight_decay=0.1)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n",
    "\n",
    "# Training\n",
    "history = []\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    # Train\n",
    "    trm.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch in train_dl:\n",
    "        features = batch['features'].to(device)\n",
    "        labels = batch['solver_label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = trm(features, return_all_cycles=True)\n",
    "        \n",
    "        # Deep supervision loss\n",
    "        loss = sum(\n",
    "            F.cross_entropy(logits, labels)\n",
    "            for logits in outputs['all_solver_logits']\n",
    "        ) / len(outputs['all_solver_logits'])\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(trm.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * features.size(0)\n",
    "        preds = outputs['solver_logits'].argmax(dim=-1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += features.size(0)\n",
    "    \n",
    "    # Eval\n",
    "    trm.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dl:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['solver_label'].to(device)\n",
    "            \n",
    "            outputs = trm(features)\n",
    "            preds = outputs['solver_logits'].argmax(dim=-1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += features.size(0)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    train_acc = train_correct / train_total\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    history.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss / train_total,\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "    })\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(trm.state_dict(), DATA_DIR / 'trm_router_best.pt')\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {train_loss/train_total:.4f} | \"\n",
    "              f\"Train: {train_acc:.4f} | Val: {val_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs = [h['epoch'] for h in history]\n",
    "\n",
    "ax1.plot(epochs, [h['train_loss'] for h in history])\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('TRM Router Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(epochs, [h['train_acc'] for h in history], label='Train')\n",
    "ax2.plot(epochs, [h['val_acc'] for h in history], label='Val')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('TRM Router Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'trm_training.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Analysis\n",
    "\n",
    "Analyze what the TRM learned about solver selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "trm.load_state_dict(torch.load(DATA_DIR / 'trm_router_best.pt'))\n",
    "trm.eval()\n",
    "\n",
    "# Confusion matrix\n",
    "SOLVER_NAMES = ['spectral_clip', 'dual_ascent', 'quasi_newton', 'frank_wolfe', 'admm']\n",
    "confusion = torch.zeros(5, 5, dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dl:\n",
    "        features = batch['features'].to(device)\n",
    "        labels = batch['solver_label'].to(device)\n",
    "        \n",
    "        outputs = trm(features)\n",
    "        preds = outputs['solver_logits'].argmax(dim=-1)\n",
    "        \n",
    "        for t, p in zip(labels.cpu(), preds.cpu()):\n",
    "            confusion[t, p] += 1\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(confusion.float() / confusion.sum(dim=1, keepdim=True), cmap='Blues')\n",
    "\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_yticks(range(5))\n",
    "ax.set_xticklabels([s[:8] for s in SOLVER_NAMES], rotation=45, ha='right')\n",
    "ax.set_yticklabels([s[:8] for s in SOLVER_NAMES])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True (Oracle)')\n",
    "ax.set_title('TRM Routing Confusion Matrix')\n",
    "\n",
    "# Add values\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        val = confusion[i, j].item()\n",
    "        color = 'white' if confusion[i, j] > confusion.max() / 2 else 'black'\n",
    "        ax.text(j, i, f'{val}', ha='center', va='center', color=color, fontsize=10)\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Data Collection**: Collecting training dynamics from runs with fixed solvers\n",
    "2. **Oracle Labeling**: Determining which solver was best at each step\n",
    "3. **TRM Training**: Training a tiny recursive model to predict optimal solver\n",
    "4. **Evaluation**: Measuring TRM's ability to match the oracle\n",
    "\n",
    "Key findings:\n",
    "- Different solvers are optimal at different training phases\n",
    "- TRM can learn to approximate the oracle solver selection\n",
    "- The TRM overhead is negligible (~100K parameters)\n",
    "\n",
    "This connects to the broader Manifold Muon project by exploring whether\n",
    "meta-learned routing can improve hyperparameter transfer properties."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
